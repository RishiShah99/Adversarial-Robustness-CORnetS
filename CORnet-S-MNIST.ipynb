{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":88937,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":74619,"modelId":99381},{"sourceId":142512,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":120729,"modelId":143931},{"sourceId":458095,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":371342,"modelId":392245}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shahrish99/cornet-s-pgd-with-mnist?scriptVersionId=256937293\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-07-04T03:25:32.224193Z","iopub.execute_input":"2025-07-04T03:25:32.22484Z","iopub.status.idle":"2025-07-04T03:25:32.53048Z","shell.execute_reply.started":"2025-07-04T03:25:32.224808Z","shell.execute_reply":"2025-07-04T03:25:32.529621Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/cornet-s_mnist_redo/pytorch/default/1/cornet_s-mnist-redo.pth\n/kaggle/input/cornet-s/pytorch/default/1/cornet_s-1d3f7974.pth\n/kaggle/input/cornet-s-mnist/pytorch/default/1/cornet_s_MNIST.pth\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Loading the dataset with labels","metadata":{}},{"cell_type":"code","source":"!pip install torchattacks","metadata":{"execution":{"iopub.status.busy":"2025-07-04T03:25:33.211356Z","iopub.execute_input":"2025-07-04T03:25:33.212162Z","iopub.status.idle":"2025-07-04T03:25:44.780635Z","shell.execute_reply.started":"2025-07-04T03:25:33.212125Z","shell.execute_reply":"2025-07-04T03:25:44.779513Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torchattacks\n  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\nRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (2.4.0)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (0.19.0)\nRequirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (1.14.0)\nRequirement already satisfied: tqdm>=4.56.1 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (4.66.4)\nCollecting requests~=2.25.1 (from torchattacks)\n  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: numpy>=1.19.4 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (1.26.4)\nCollecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks) (2024.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.2->torchattacks) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\nDownloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: idna, chardet, requests, torchattacks\n  Attempting uninstall: idna\n    Found existing installation: idna 3.7\n    Uninstalling idna-3.7:\n      Successfully uninstalled idna-3.7\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Uninstalling requests-2.32.3:\n      Successfully uninstalled requests-2.32.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\nbigframes 0.22.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\nconda 24.7.1 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\nconda 24.7.1 requires requests<3,>=2.28.0, but you have requests 2.25.1 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\ndatasets 2.21.0 requires requests>=2.32.2, but you have requests 2.25.1 which is incompatible.\ndocker 7.1.0 requires requests>=2.26.0, but you have requests 2.25.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-server 2.27.2 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires requests>=2.27, but you have requests 2.25.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires requests<2.33,>=2.27, but you have requests 2.25.1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport torchattacks","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:02.980761Z","iopub.execute_input":"2025-04-08T21:34:02.981032Z","iopub.status.idle":"2025-04-08T21:34:08.423637Z","shell.execute_reply.started":"2025-04-08T21:34:02.981008Z","shell.execute_reply":"2025-04-08T21:34:08.422967Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define transformations for MNIST\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=3),\n    transforms.Resize((224, 224)),  # Resize for CORNet-S input\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))  # Normalize MNIST\n])","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:08.424928Z","iopub.execute_input":"2025-04-08T21:34:08.425702Z","iopub.status.idle":"2025-04-08T21:34:08.430616Z","shell.execute_reply.started":"2025-04-08T21:34:08.425636Z","shell.execute_reply":"2025-04-08T21:34:08.429639Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load MNIST dataset\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:08.432061Z","iopub.execute_input":"2025-04-08T21:34:08.432331Z","iopub.status.idle":"2025-04-08T21:34:10.713958Z","shell.execute_reply.started":"2025-04-08T21:34:08.432287Z","shell.execute_reply":"2025-04-08T21:34:10.713015Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 38308247.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 1169094.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 10711119.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 3448059.51it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:10.714994Z","iopub.execute_input":"2025-04-08T21:34:10.715245Z","iopub.status.idle":"2025-04-08T21:34:10.779161Z","shell.execute_reply.started":"2025-04-08T21:34:10.715221Z","shell.execute_reply":"2025-04-08T21:34:10.778258Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!git clone https://github.com/dicarlolab/CORnet.git","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:10.782163Z","iopub.execute_input":"2025-04-08T21:34:10.782511Z","iopub.status.idle":"2025-04-08T21:34:12.424081Z","shell.execute_reply.started":"2025-04-08T21:34:10.782474Z","shell.execute_reply":"2025-04-08T21:34:12.422986Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'CORnet'...\nremote: Enumerating objects: 155, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 155 (delta 13), reused 9 (delta 8), pack-reused 135 (from 1)\u001b[K\nReceiving objects: 100% (155/155), 68.11 KiB | 1.55 MiB/s, done.\nResolving deltas: 100% (87/87), done.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Navigate to the cloned repository folder\nimport os\nos.chdir('/kaggle/working/CORnet')\n\n# Install the package if needed\n!pip install .","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:12.425539Z","iopub.execute_input":"2025-04-08T21:34:12.425872Z","iopub.status.idle":"2025-04-08T21:34:25.373212Z","shell.execute_reply.started":"2025-04-08T21:34:12.425842Z","shell.execute_reply":"2025-04-08T21:34:25.372025Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Processing /kaggle/working/CORnet\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (0.19.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (4.66.4)\nCollecting fire (from CORnet==0.1.0)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (2024.6.1)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->CORnet==0.1.0) (2.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->CORnet==0.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->CORnet==0.1.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->CORnet==0.1.0) (2024.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->CORnet==0.1.0) (9.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->CORnet==0.1.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->CORnet==0.1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->CORnet==0.1.0) (1.3.0)\nBuilding wheels for collected packages: CORnet, fire\n  Building wheel for CORnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for CORnet: filename=CORnet-0.1.0-py3-none-any.whl size=23228 sha256=30a5b76f7ec24bbfba3c6d8fcd3d2c51dfa27c8445a9a5e3e6e7837db0d33361\n  Stored in directory: /tmp/pip-ephem-wheel-cache-dg40u8e3/wheels/ab/bb/f9/8716bf8cc3f23c0cd07d33b31c64ed0bc87023663d6be90ad5\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=99da034ed40926b63bd55b320744e336f598453a1d63bc753135e323603d0f76\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built CORnet fire\nInstalling collected packages: fire, CORnet\nSuccessfully installed CORnet-0.1.0 fire-0.7.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Import the CORnet-S model architecture\nfrom cornet import cornet_s\n\n# Initialize the CORnet-S model\nmodel = cornet_s()\n\nprint(model) # Print the model","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:25.374838Z","iopub.execute_input":"2025-04-08T21:34:25.375749Z","iopub.status.idle":"2025-04-08T21:34:26.324237Z","shell.execute_reply.started":"2025-04-08T21:34:25.375703Z","shell.execute_reply":"2025-04-08T21:34:26.323318Z"},"trusted":true},"outputs":[{"name":"stdout","text":"DataParallel(\n  (module): Sequential(\n    (V1): Sequential(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (nonlin1): ReLU(inplace=True)\n      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (nonlin2): ReLU(inplace=True)\n      (output): Identity()\n    )\n    (V2): CORblock_S(\n      (conv_input): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (V4): CORblock_S(\n      (conv_input): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (IT): CORblock_S(\n      (conv_input): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (decoder): Sequential(\n      (avgpool): AdaptiveAvgPool2d(output_size=1)\n      (flatten): Flatten()\n      (linear): Linear(in_features=512, out_features=1000, bias=True)\n      (output): Identity()\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torch.nn as nn\n\n# Load pre-trained weights without modifying the final layer\ncheckpoint = torch.load(\"/kaggle/input/cornet-s/pytorch/default/1/cornet_s-1d3f7974.pth\", map_location=device)\nmodel.load_state_dict(checkpoint['state_dict'], strict=False)  # Use strict=False to ignore final layer mismatch","metadata":{"execution":{"iopub.status.busy":"2024-12-14T22:33:21.42055Z","iopub.execute_input":"2024-12-14T22:33:21.420801Z","iopub.status.idle":"2024-12-14T22:33:23.977884Z","shell.execute_reply.started":"2024-12-14T22:33:21.420775Z","shell.execute_reply":"2024-12-14T22:33:23.977032Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training procedure","metadata":{}},{"cell_type":"code","source":"# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-12-14T22:33:35.831223Z","iopub.execute_input":"2024-12-14T22:33:35.8321Z","iopub.status.idle":"2024-12-14T22:33:35.836747Z","shell.execute_reply.started":"2024-12-14T22:33:35.832048Z","shell.execute_reply":"2024-12-14T22:33:35.835884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary modules\nimport time\nfrom tqdm import tqdm\n\n# Move model to the appropriate device\nmodel = model.to(device)\n\n# Training loop\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    model.train()  # Set model to training mode\n    running_loss = 0.0\n\n    # Training phase\n    for i, (inputs, labels) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()  # Zero the parameter gradients\n\n        outputs = model(inputs)  # Forward pass\n        loss = criterion(outputs, labels)  # Compute loss\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n\n        running_loss += loss.item() * inputs.size(0)  # Update running loss\n\n        if i % 100 == 0:  # Print every 100 batches\n            print(f\"Step [{i}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    epoch_loss = running_loss / len(train_loader.dataset)  # Compute epoch loss\n\n    # Validation phase\n    model.eval()  # Set model to evaluation mode\n    val_loss = 0.0\n    correct1 = 0\n    correct5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)  # Forward pass\n            loss = criterion(outputs, labels)  # Compute loss\n\n            val_loss += loss.item() * inputs.size(0)  # Update validation loss\n\n            # Calculate top-1 and top-5 accuracy\n            _, pred1 = outputs.topk(1, 1, True, True)\n            _, pred5 = outputs.topk(5, 1, True, True)\n            correct1 += pred1.eq(labels.view(-1, 1).expand_as(pred1)).sum().item()\n            correct5 += pred5.eq(labels.view(-1, 1).expand_as(pred5)).sum().item()\n            total += labels.size(0)\n\n    epoch_val_loss = val_loss / len(test_loader.dataset)  # Compute epoch validation loss\n    top1_acc = correct1 / total  # Compute top-1 accuracy\n    top5_acc = correct5 / total  # Compute top-5 accuracy\n\n    end_time = time.time()\n    epoch_duration = end_time - start_time\n\n    print(f'Epoch [{epoch+1}/{num_epochs}] - Time: {epoch_duration:.2f}s')\n    print(f'Train Loss: {epoch_loss:.4f}')\n    print(f'Validation Loss: {epoch_val_loss:.4f}')\n    print(f'Top-1 Accuracy: {top1_acc:.4f}, Top-5 Accuracy: {top5_acc:.4f}')\n\n    # Save the model checkpoint\n    model_save_path = f'/kaggle/working/cornet_s_epoch{epoch+1}.pth'\n    torch.save(model.state_dict(), model_save_path)\n    print(f'Model saved at {model_save_path}')\n\nprint('Training complete')","metadata":{"execution":{"iopub.status.busy":"2024-10-07T18:53:22.127966Z","iopub.execute_input":"2024-10-07T18:53:22.128348Z","iopub.status.idle":"2024-10-07T19:08:09.552909Z","shell.execute_reply.started":"2024-10-07T18:53:22.128313Z","shell.execute_reply":"2024-10-07T19:08:09.551617Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the model checkpoint\nmodel_path = '/kaggle/working/cornet_s_epoch1.pth'\nmodel.load_state_dict(torch.load(model_path))","metadata":{"execution":{"iopub.status.busy":"2024-10-07T19:08:12.986433Z","iopub.execute_input":"2024-10-07T19:08:12.986816Z","iopub.status.idle":"2024-10-07T19:08:13.182768Z","shell.execute_reply.started":"2024-10-07T19:08:12.986779Z","shell.execute_reply":"2024-10-07T19:08:13.181843Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing model out with some images:\nimport matplotlib.pyplot as plt\nimport torch\n\ndef show_images(images, labels, preds=None, num_images=6):\n    \"\"\"\n    Function to display a grid of images with corresponding labels and predictions.\n    \"\"\"\n    images = images[:num_images]\n    labels = labels[:num_images]\n\n    # Create a grid of images\n    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n    for i in range(num_images):\n        # Convert the tensor to a NumPy array for displaying\n        img = images[i].cpu().numpy().transpose((1, 2, 0))\n        axes[i].imshow(img, cmap='gray' if img.shape[2] == 1 else None)\n        axes[i].axis('off')\n\n        # Display label and prediction\n        title = f\"Label: {labels[i].item()}\"\n        if preds is not None:\n            title += f\"\\nPred: {preds[i].item()}\"\n        axes[i].set_title(title)\n\n    plt.show()\n\n# Example usage after training the model on clean images:\nmodel.eval()\ndata_iter = iter(test_loader)\nimages, labels = next(data_iter)\n\n# Run the model on the clean images\noutputs = model(images.to(device))\n_, preds = torch.max(outputs, 1)\n\n# Visualize the clean images and predictions\nshow_images(images, labels, preds)","metadata":{"execution":{"iopub.status.busy":"2024-10-21T21:37:03.637471Z","iopub.execute_input":"2024-10-21T21:37:03.638131Z","iopub.status.idle":"2024-10-21T21:37:04.404362Z","shell.execute_reply.started":"2024-10-21T21:37:03.638092Z","shell.execute_reply":"2024-10-21T21:37:04.403475Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PGD Attacks","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\n# Load pre-trained weights without modifying the final layer\ncheckpoint = torch.load(\"/kaggle/input/cornet-s-mnist/pytorch/default/1/cornet_s_MNIST.pth\", map_location=device)\n\nmodel.load_state_dict(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2025-04-08T21:34:26.562011Z","iopub.execute_input":"2025-04-08T21:34:26.562362Z","iopub.status.idle":"2025-04-08T21:34:29.484039Z","shell.execute_reply.started":"2025-04-08T21:34:26.562333Z","shell.execute_reply":"2025-04-08T21:34:29.48316Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/2349068480.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/input/cornet-s-mnist/pytorch/default/1/cornet_s_MNIST.pth\", map_location=device)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# PGD Attack\npgd_attack = torchattacks.PGD(model, eps=0.001, alpha=2/255, steps=40)\n\n# Function to test adversarial accuracy\ndef adversarial_test(attack, loader, attack_name):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    for inputs, labels in tqdm(loader, desc=f\"Adversarial Test ({attack_name})\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        \n        _, pred = outputs.max(1)\n        correct += pred.eq(labels).sum().item()\n        total += labels.size(0)\n    \n    acc = correct / total\n    print(f'{attack_name} Accuracy: {acc:.4f}')\n    return acc\n\n# Evaluate model with PGD attack\npgd_acc = adversarial_test(pgd_attack, test_loader, \"PGD\")","metadata":{"execution":{"iopub.status.busy":"2025-07-04T03:25:17.281004Z","iopub.execute_input":"2025-07-04T03:25:17.281225Z","iopub.status.idle":"2025-07-04T03:25:17.613987Z","shell.execute_reply.started":"2025-07-04T03:25:17.2812Z","shell.execute_reply":"2025-07-04T03:25:17.612849Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# PGD Attack\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pgd_attack \u001b[38;5;241m=\u001b[39m \u001b[43mtorchattacks\u001b[49m\u001b[38;5;241m.\u001b[39mPGD(model, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to test adversarial accuracy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madversarial_test\u001b[39m(attack, loader, attack_name):\n","\u001b[0;31mNameError\u001b[0m: name 'torchattacks' is not defined"],"ename":"NameError","evalue":"name 'torchattacks' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# PGD Attack for Adversarial Training\npgd_attack_train = torchattacks.PGD(model, eps=0.3, alpha=2/255, steps=40)\n\n# Function for Adversarial Training with PGD\ndef adversarial_train_pgd(loader, model, optimizer, criterion, attack):\n    model.train()\n    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n    \n    for i, (inputs, labels) in progress_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Generate PGD adversarial examples\n        adv_inputs = attack(inputs, labels)\n        \n        # Forward pass with adversarial inputs\n        optimizer.zero_grad()\n        outputs = model(adv_inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Update progress bar with loss\n        progress_bar.set_postfix(loss=loss.item())\n\n# Training loop (e.g., 10 epochs)\nfor epoch in range(10):\n    adversarial_train_pgd(train_loader, model, optimizer, criterion, pgd_attack_train)\n    print(f\"Epoch {epoch+1} completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T23:30:19.381361Z","iopub.execute_input":"2024-10-06T23:30:19.38167Z","iopub.status.idle":"2024-10-06T23:39:51.990624Z","shell.execute_reply.started":"2024-10-06T23:30:19.381637Z","shell.execute_reply":"2024-10-06T23:39:51.989291Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Re-test model against clean data and PGD attack\n\n# Test on clean data\nclean_accuracy = adversarial_test(None, test_loader, \"Clean\")\nprint(f'Accuracy on clean data after PGD adversarial training: {clean_accuracy * 100:.2f}%')\n\n# Test on PGD adversarial examples\npgd_acc_after = adversarial_test(pgd_attack, test_loader, \"PGD\")\nprint(f'Accuracy under PGD Attack after adversarial training: {pgd_acc_after * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T23:39:51.991354Z","iopub.status.idle":"2024-10-06T23:39:51.991703Z","shell.execute_reply.started":"2024-10-06T23:39:51.991532Z","shell.execute_reply":"2024-10-06T23:39:51.99155Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Adversarial FGSM Training and Attacks\n* This section will conduct FGSM attacks, and then we will make the model more robust by training it with adversarial attacks, and then retest it with FGSM attacks.\n* Issue with this is that FGSM attacks are not effective against this model, it still has a 95% accuracy. We will go back to PGD attacks then. ","metadata":{}},{"cell_type":"markdown","source":"## FGSM Attacks\n* We first need to see the accuracy that CORnet-S can achieve with FGSM attacks.\n","metadata":{}},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\n\n# Define FGSM attack\nfgsm = torchattacks.FGSM(model, eps=0.3)\n\n# Function to evaluate under FGSM attack with TQDM\ndef evaluate_under_attack(loader, model, attack):\n    model.eval()\n    correct = 0\n    total = 0\n    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"Evaluating\")\n    for i, (inputs, labels) in progress_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Generate adversarial examples if attack is specified\n        if attack:\n            adv_inputs = attack(inputs, labels)\n        else:\n            adv_inputs = inputs\n        \n        outputs = model(adv_inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        \n        # Update the progress bar with current accuracy\n        progress_bar.set_postfix(batch_accuracy=(correct / total))\n    \n    return correct / total\n\n# Test the model under FGSM attack\naccuracy_fgsm = evaluate_under_attack(test_loader, model, fgsm)\nprint(f'Accuracy under FGSM Attack: {accuracy_fgsm * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-12-14T22:34:08.303704Z","iopub.execute_input":"2024-12-14T22:34:08.304558Z","iopub.status.idle":"2024-12-14T22:36:19.585457Z","shell.execute_reply.started":"2024-12-14T22:34:08.304522Z","shell.execute_reply":"2024-12-14T22:36:19.584334Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Adversarial Training","metadata":{}},{"cell_type":"code","source":"# Function for adversarial training with progress bar\ndef adversarial_train(loader, model, optimizer, criterion, attack):\n    model.train()\n    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"Training\")\n    for i, (inputs, labels) in progress_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Generate adversarial examples using FGSM\n        adv_inputs = attack(inputs, labels)\n        \n        # Forward pass with adversarial inputs\n        optimizer.zero_grad()\n        outputs = model(adv_inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update progress bar with loss\n        progress_bar.set_postfix(loss=loss.item())\n\n# Adversarial training loop (e.g., 10 epochs)\nfor epoch in range(10):\n    adversarial_train(train_loader, model, optimizer, criterion, fgsm)\n    print(f\"Epoch {epoch+1} completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-06T22:30:03.530939Z","iopub.execute_input":"2024-10-06T22:30:03.531482Z","iopub.status.idle":"2024-10-06T22:35:32.309644Z","shell.execute_reply.started":"2024-10-06T22:30:03.531435Z","shell.execute_reply":"2024-10-06T22:35:32.30847Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Re-Test","metadata":{}},{"cell_type":"code","source":"# Evaluate on clean data after adversarial training\nclean_accuracy = evaluate_under_attack(test_loader, model, None)\nprint(f'Accuracy on clean data after FGSM adversarial training: {clean_accuracy * 100:.2f}%')\n\n# Evaluate on FGSM attack again after adversarial training\naccuracy_fgsm_after = evaluate_under_attack(test_loader, model, fgsm)\nprint(f'Accuracy under FGSM Attack after adversarial training: {accuracy_fgsm_after * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-10-06T22:35:32.310384Z","iopub.status.idle":"2024-10-06T22:35:32.310744Z","shell.execute_reply.started":"2024-10-06T22:35:32.310566Z","shell.execute_reply":"2024-10-06T22:35:32.310585Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CW Attacks and Training\n* Since the model is scoring so well against FGSM and PGD attacks, we can use CW attacks to find a bad accuracy and then improve it with adversarial training.","metadata":{}},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\n\n# CW Attack Setup\ncw_attack = torchattacks.CW(model, c=1e-3, kappa=0, steps=100, lr=0.01)\n\n# Function to test model accuracy on CW attack with TQDM\ndef adversarial_test(attack, loader, attack_name):\n    model.eval()  # Set model to evaluation mode\n    correct = 0\n    total = 0\n    \n    # Iterate through the data loader with a progress bar\n    for inputs, labels in tqdm(loader, desc=f\"Adversarial Test ({attack_name})\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Generate adversarial examples\n        adv_inputs = attack(inputs, labels)\n        \n        # Get model outputs from adversarial inputs\n        outputs = model(adv_inputs)\n        _, pred = outputs.max(1)  # Get predicted class\n        \n        correct += pred.eq(labels).sum().item()  # Count correct predictions\n        total += labels.size(0)  # Total number of labels\n    \n    # Calculate accuracy\n    acc = correct / total\n    print(f'{attack_name} Accuracy: {acc:.4f}')\n    return acc\n\n# Test model accuracy on CW attack\ncw_acc = adversarial_test(cw_attack, test_loader, \"CW\")\nprint(f'Accuracy under CW Attack: {cw_acc * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2025-04-09T01:34:36.792339Z","iopub.execute_input":"2025-04-09T01:34:36.793003Z","iopub.status.idle":"2025-04-09T01:59:51.654786Z","shell.execute_reply.started":"2025-04-09T01:34:36.792966Z","shell.execute_reply":"2025-04-09T01:59:51.653746Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Adversarial Test (CW): 100%|██████████| 157/157 [25:14<00:00,  9.65s/it]","output_type":"stream"},{"name":"stdout","text":"CW Accuracy: 0.9712\nAccuracy under CW Attack: 97.12%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# CW Adversarial Training Function\ndef adversarial_train_cw(loader, model, optimizer, criterion, attack):\n    model.train()\n    progress_bar = tqdm(enumerate(loader), total=len(loader), desc=\"CW Adversarial Training\")\n    \n    for i, (inputs, labels) in progress_bar:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        # Generate CW adversarial examples\n        adv_inputs = attack(inputs, labels)\n        \n        # Forward pass with adversarial inputs\n        optimizer.zero_grad()\n        outputs = model(adv_inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n        \n        # Update progress bar with loss\n        progress_bar.set_postfix(loss=loss.item())\n\n# Train model on CW adversarial examples\nfor epoch in range(10):  # Adjust number of epochs based on needs\n    adversarial_train_cw(train_loader, model, optimizer, criterion, cw_attack)\n    print(f\"Epoch {epoch+1} completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:04:34.615117Z","iopub.execute_input":"2024-10-07T20:04:34.61585Z","iopub.status.idle":"2024-10-07T20:04:56.24745Z","shell.execute_reply.started":"2024-10-07T20:04:34.61581Z","shell.execute_reply":"2024-10-07T20:04:56.246103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Re-Test model on CW after training\ncw_acc_after = adversarial_test(cw_attack, test_loader, \"CW After Training\")\nprint(f'Accuracy under CW Attack after adversarial training: {cw_acc_after * 100:.2f}%')\n\n# Show CW adversarial examples again after training\nshow_adversarial_examples(test_loader, cw_attack, num_images=5)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-07T20:04:56.24816Z","iopub.status.idle":"2024-10-07T20:04:56.248516Z","shell.execute_reply.started":"2024-10-07T20:04:56.248334Z","shell.execute_reply":"2024-10-07T20:04:56.248352Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Redo","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\n!git clone https://github.com/dicarlolab/CORnet.git\n\n# Navigate to the cloned repository folder\nimport os\nos.chdir('/kaggle/working/CORnet')\n\n# Install the package if needed\n!pip install .\n\n# Import the CORnet-S model architecture\nfrom cornet import cornet_s\n# CORnet-S model\nmodel = cornet_s(pretrained=False).to(device)\nmodel.module.decoder.linear = nn.Linear(model.module.decoder.linear.in_features, 10).to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:34:13.343892Z","iopub.execute_input":"2025-07-04T16:34:13.344261Z","iopub.status.idle":"2025-07-04T16:34:34.860538Z","shell.execute_reply.started":"2025-07-04T16:34:13.344225Z","shell.execute_reply":"2025-07-04T16:34:34.859424Z"}},"outputs":[{"name":"stdout","text":"cuda\nCloning into 'CORnet'...\nremote: Enumerating objects: 155, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 155 (delta 13), reused 9 (delta 8), pack-reused 135 (from 1)\u001b[K\nReceiving objects: 100% (155/155), 68.11 KiB | 2.62 MiB/s, done.\nResolving deltas: 100% (87/87), done.\nProcessing /kaggle/working/CORnet\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (2.4.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (0.19.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (2.2.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from CORnet==0.1.0) (4.66.4)\nCollecting fire (from CORnet==0.1.0)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->CORnet==0.1.0) (2024.6.1)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->CORnet==0.1.0) (2.4.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->CORnet==0.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->CORnet==0.1.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->CORnet==0.1.0) (2024.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->CORnet==0.1.0) (9.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->CORnet==0.1.0) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->CORnet==0.1.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->CORnet==0.1.0) (1.3.0)\nBuilding wheels for collected packages: CORnet, fire\n  Building wheel for CORnet (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for CORnet: filename=CORnet-0.1.0-py3-none-any.whl size=23228 sha256=a3a05822d318f21d0b4702dd4db1c758b8520a6b8d545653d52210c47e25b38e\n  Stored in directory: /tmp/pip-ephem-wheel-cache-u5p8bb3q/wheels/ab/bb/f9/8716bf8cc3f23c0cd07d33b31c64ed0bc87023663d6be90ad5\n  Building wheel for fire (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=ef76b328cbe6bc8dab7b7f7dfc50aaed152c098567054fa63c91be867f74b381\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built CORnet fire\nInstalling collected packages: fire, CORnet\nSuccessfully installed CORnet-0.1.0 fire-0.7.0\nDataParallel(\n  (module): Sequential(\n    (V1): Sequential(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (nonlin1): ReLU(inplace=True)\n      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (nonlin2): ReLU(inplace=True)\n      (output): Identity()\n    )\n    (V2): CORblock_S(\n      (conv_input): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (V4): CORblock_S(\n      (conv_input): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (IT): CORblock_S(\n      (conv_input): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (decoder): Sequential(\n      (avgpool): AdaptiveAvgPool2d(output_size=1)\n      (flatten): Flatten()\n      (linear): Linear(in_features=512, out_features=10, bias=True)\n      (output): Identity()\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# --- Transforms for MNIST (resize + convert to 3-channel for CORNet-S) ---\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),              # CORNet-S expects larger images\n    transforms.Grayscale(num_output_channels=3),# Convert 1-channel MNIST to 3\n    transforms.ToTensor()\n])\n\n# --- Datasets ---\ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\ntest_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n# --- Dataloaders ---\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\n# --- Loss and Optimizer ---\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# --- Training Function ---\ndef train(model, device, train_loader, optimizer, epoch, criterion):\n    model.train()\n    pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch}\")\n    total_loss, correct, total = 0, 0, 0\n\n    for data, target in pbar:\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * data.size(0)\n        pred = output.argmax(dim=1)\n        correct += pred.eq(target).sum().item()\n        total += target.size(0)\n\n        pbar.set_postfix(loss=total_loss/total, acc=correct/total)\n\n# --- Testing Function ---\ndef test(model, device, test_loader, criterion):\n    model.eval()\n    pbar = tqdm(test_loader, desc=\"Testing\")\n    total_loss, correct, total = 0, 0, 0\n\n    with torch.no_grad():\n        for data, target in pbar:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output, target)\n\n            total_loss += loss.item() * data.size(0)\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n            total += target.size(0)\n\n            pbar.set_postfix(loss=total_loss/total, acc=correct/total)\n\n    return total_loss / total, correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:35:06.283763Z","iopub.execute_input":"2025-07-04T16:35:06.28428Z","iopub.status.idle":"2025-07-04T16:35:08.617554Z","shell.execute_reply.started":"2025-07-04T16:35:06.284244Z","shell.execute_reply":"2025-07-04T16:35:08.616065Z"}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 34414216.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 1042987.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 9427101.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 2883385.62it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"for epoch in range(1, 11):\n    train(model, device, train_loader, optimizer, epoch, criterion)\n    test(model, device, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T22:18:49.216533Z","iopub.execute_input":"2025-07-03T22:18:49.216855Z","iopub.status.idle":"2025-07-03T23:23:06.422401Z","shell.execute_reply.started":"2025-07-03T22:18:49.216828Z","shell.execute_reply":"2025-07-03T23:23:06.421204Z"}},"outputs":[{"name":"stderr","text":"Train Epoch 1:   0%|          | 0/938 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nTrain Epoch 1: 100%|██████████| 938/938 [15:11<00:00,  1.03it/s, acc=0.964, loss=0.123]\nTesting: 100%|██████████| 10/10 [00:48<00:00,  4.87s/it, acc=0.984, loss=0.0486]\nTrain Epoch 2: 100%|██████████| 938/938 [15:08<00:00,  1.03it/s, acc=0.984, loss=0.0505]\nTesting: 100%|██████████| 10/10 [00:47<00:00,  4.78s/it, acc=0.987, loss=0.0403]\nTrain Epoch 3: 100%|██████████| 938/938 [15:08<00:00,  1.03it/s, acc=0.987, loss=0.0401]\nTesting: 100%|██████████| 10/10 [00:48<00:00,  4.82s/it, acc=0.978, loss=0.0693]\nTrain Epoch 4: 100%|██████████| 938/938 [15:25<00:00,  1.01it/s, acc=0.989, loss=0.0359]\nTesting: 100%|██████████| 10/10 [00:51<00:00,  5.16s/it, acc=0.99, loss=0.0316]\nTrain Epoch 5:   1%|          | 5/938 [00:05<18:39,  1.20s/it, acc=0.991, loss=0.0281]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     test(model, device, test_loader, criterion)\n","Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, criterion)\u001b[0m\n\u001b[1;32m     36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m data\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"!pip install torchattacks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:35:16.076927Z","iopub.execute_input":"2025-07-04T16:35:16.077269Z","iopub.status.idle":"2025-07-04T16:35:26.157167Z","shell.execute_reply.started":"2025-07-04T16:35:16.077244Z","shell.execute_reply":"2025-07-04T16:35:26.15636Z"}},"outputs":[{"name":"stdout","text":"Collecting torchattacks\n  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\nRequirement already satisfied: torch>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (2.4.0)\nRequirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (0.19.0)\nRequirement already satisfied: scipy>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (1.14.0)\nRequirement already satisfied: tqdm>=4.56.1 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (4.66.4)\nCollecting requests~=2.25.1 (from torchattacks)\n  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: numpy>=1.19.4 in /opt/conda/lib/python3.10/site-packages (from torchattacks) (1.26.4)\nCollecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.25.1->torchattacks) (2024.7.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (1.13.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7.1->torchattacks) (2024.6.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.8.2->torchattacks) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7.1->torchattacks) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7.1->torchattacks) (1.3.0)\nDownloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: idna, chardet, requests, torchattacks\n  Attempting uninstall: idna\n    Found existing installation: idna 3.7\n    Uninstalling idna-3.7:\n      Successfully uninstalled idna-3.7\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Uninstalling requests-2.32.3:\n      Successfully uninstalled requests-2.32.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbeatrix-jupyterlab 2024.66.154055 requires jupyterlab~=3.6.0, but you have jupyterlab 4.2.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.10.0, but you have google-cloud-bigquery 2.34.4 which is incompatible.\nbigframes 0.22.0 requires google-cloud-storage>=2.0.0, but you have google-cloud-storage 1.44.0 which is incompatible.\nbigframes 0.22.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.2 which is incompatible.\nbigframes 0.22.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\nconda 24.7.1 requires packaging>=23.0, but you have packaging 21.3 which is incompatible.\nconda 24.7.1 requires requests<3,>=2.28.0, but you have requests 2.25.1 which is incompatible.\ndataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.8.2 which is incompatible.\ndatasets 2.21.0 requires requests>=2.32.2, but you have requests 2.25.1 which is incompatible.\ndocker 7.1.0 requires requests>=2.26.0, but you have requests 2.25.1 which is incompatible.\ngoogle-cloud-aiplatform 0.6.0a1 requires google-api-core[grpc]<2.0.0dev,>=1.22.2, but you have google-api-core 2.11.1 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.11.1 which is incompatible.\njupyterlab 4.2.4 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-server 2.27.2 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires requests>=2.27, but you have requests 2.25.1 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.2 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.4 requires requests<2.33,>=2.27, but you have requests 2.25.1 which is incompatible.\nosmnx 1.9.4 requires shapely<2.1,>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.5.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.9.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:35:37.529552Z","iopub.execute_input":"2025-07-04T16:35:37.530522Z","iopub.status.idle":"2025-07-04T16:35:37.535098Z","shell.execute_reply.started":"2025-07-04T16:35:37.530486Z","shell.execute_reply":"2025-07-04T16:35:37.534118Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/cornet_s-mnist-redo.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T23:33:51.12426Z","iopub.execute_input":"2025-07-03T23:33:51.124617Z","iopub.status.idle":"2025-07-03T23:33:51.56196Z","shell.execute_reply.started":"2025-07-03T23:33:51.124585Z","shell.execute_reply":"2025-07-03T23:33:51.561238Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch.nn as nn\n\n# Load pre-trained weights without modifying the final layer\ncheckpoint = torch.load(\"/kaggle/input/cornet-s_mnist_redo/pytorch/default/1/cornet_s-mnist-redo.pth\", map_location=device)\n\nmodel.load_state_dict(checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:35:43.143661Z","iopub.execute_input":"2025-07-04T16:35:43.143988Z","iopub.status.idle":"2025-07-04T16:35:45.901621Z","shell.execute_reply.started":"2025-07-04T16:35:43.143959Z","shell.execute_reply":"2025-07-04T16:35:45.900811Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_36/3750294667.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/input/cornet-s_mnist_redo/pytorch/default/1/cornet_s-mnist-redo.pth\", map_location=device)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\n\n\n# --- PGD Attack Configuration ---\npgd_eps = 0.001     \npgd_alpha = 2/255\npgd_steps = 40\n\npgd_attack = torchattacks.PGD(model, eps=pgd_eps, alpha=pgd_alpha, steps=pgd_steps)\n\n# --- PGD Adversarial Accuracy Function ---\ndef adversarial_test_pgd(attack, loader):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(loader, desc=\"Adversarial Test (PGD)\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n    acc = correct / total\n    print(f\"📊 PGD Accuracy (ε={pgd_eps}): {acc:.4f}\")\n    return acc\n\n# --- Run PGD Evaluation ---\npgd_acc = adversarial_test_pgd(pgd_attack, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T03:26:24.767289Z","iopub.execute_input":"2025-07-04T03:26:24.767637Z"}},"outputs":[{"name":"stderr","text":"Adversarial Test (PGD):   0%|          | 0/157 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\nAdversarial Test (PGD):  32%|███▏      | 50/157 [19:05<41:55, 23.51s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"test(model, device, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:36:56.482478Z","iopub.execute_input":"2025-07-04T16:36:56.483169Z","iopub.status.idle":"2025-07-04T16:37:43.772814Z","shell.execute_reply.started":"2025-07-04T16:36:56.483138Z","shell.execute_reply":"2025-07-04T16:37:43.771933Z"}},"outputs":[{"name":"stderr","text":"Testing: 100%|██████████| 313/313 [00:47<00:00,  6.62it/s, acc=0.99, loss=0.0306] \n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(0.030556247400678693, 0.9899)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\n\n# ---------------- CW Attack Setup ----------------\ncw_c = 1e-1          # L2 perturbation penalty\ncw_kappa = 0         # Confidence\ncw_steps = 100\ncw_lr = 0.01\n\ncw_attack = torchattacks.CW(model, c=cw_c, kappa=cw_kappa, steps=cw_steps, lr=cw_lr)\n\n# ---------------- CW Adversarial Accuracy Function ----------------\ndef adversarial_test_cw(attack, loader):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(loader, desc=f\"Adversarial Test (CW)\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n    acc = correct / total\n    print(f\"📊 CW Accuracy (c={cw_c}, kappa={cw_kappa}): {acc:.4f}\")\n    return acc\n\n# ---------------- Run CW Evaluation ----------------\ncw_acc = adversarial_test_cw(cw_attack, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T16:37:43.774182Z","iopub.execute_input":"2025-07-04T16:37:43.774487Z"}},"outputs":[{"name":"stderr","text":"Adversarial Test (CW):  79%|███████▉  | 248/313 [23:26<06:13,  5.74s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
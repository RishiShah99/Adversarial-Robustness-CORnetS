{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":193003,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":164597,"modelId":186926}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shahrish99/cornet-s-cifar100?scriptVersionId=256937529\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Model Creation","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:52:33.085519Z","iopub.execute_input":"2024-12-08T22:52:33.086354Z","iopub.status.idle":"2024-12-08T22:52:33.094911Z","shell.execute_reply.started":"2024-12-08T22:52:33.086318Z","shell.execute_reply":"2024-12-08T22:52:33.09398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Check for GPU availability\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Hyperparameters\nBATCH_SIZE = 64\nLR = 1e-3\nNUM_EPOCHS = 10\nSTEP_SIZE = 5\nGAMMA = 0.1\n\n# Data preprocessing and augmentation\ntransform_train = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),\n    transforms.Resize(224),  # Resize CIFAR-100 images to 224x224\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet stats\n])\n\ntransform_test = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Load CIFAR-100 dataset\ntrain_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, transform=transform_train, download=True)\ntest_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, transform=transform_test, download=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n# Clone CORNet repository (specific to Google Colab)\n!git clone https://github.com/dicarlolab/CORnet.git\n\n# Navigate to the cloned repository folder\nos.chdir('/kaggle/working/CORnet')\n\n# Install the package\n!pip install .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:52:33.433452Z","iopub.execute_input":"2024-12-08T22:52:33.434161Z","iopub.status.idle":"2024-12-08T22:52:52.408636Z","shell.execute_reply.started":"2024-12-08T22:52:33.434129Z","shell.execute_reply":"2024-12-08T22:52:52.407732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# Specify the path to the saved checkpoint\ncheckpoint_path = '/kaggle/input/cornets-cifar100/pytorch/default/1/cornet_s_retrained_epoch15.pth'  # Update with the actual path after uploading\n\n# Load the model\nfrom cornet import cornet_s\n\nmodel = cornet_s()\n\n# Update the final layer for CIFAR-100\nnum_classes = 100\nmodel.module.decoder.linear = nn.Linear(\n    in_features=model.module.decoder.linear.in_features,\n    out_features=num_classes\n)\nmodel = model.to(device)\n\n# Load the saved state dictionary\nmodel.load_state_dict(torch.load(checkpoint_path))\nprint(f\"Model loaded from {checkpoint_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:52:52.410456Z","iopub.execute_input":"2024-12-08T22:52:52.410823Z","iopub.status.idle":"2024-12-08T22:52:55.087209Z","shell.execute_reply.started":"2024-12-08T22:52:52.410789Z","shell.execute_reply":"2024-12-08T22:52:55.086319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Perform a validation loop to check accuracy\nmodel.eval()\nval_loss = 0.0\ncorrect1 = 0\ncorrect5 = 0\ntotal = 0\n\nprint(\"Running validation to check initial accuracy...\")\nwith torch.no_grad():\n    for inputs, labels in tqdm(test_loader, desc=\"Validation (Pre-training Check)\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        val_loss += loss.item() * inputs.size(0)\n\n        # Calculate top-1 and top-5 accuracy\n        _, pred1 = outputs.topk(1, 1, True, True)\n        _, pred5 = outputs.topk(5, 1, True, True)\n        correct1 += pred1.eq(labels.view(-1, 1).expand_as(pred1)).sum().item()\n        correct5 += pred5.eq(labels.view(-1, 1).expand_as(pred5)).sum().item()\n        total += labels.size(0)\n\nepoch_val_loss = val_loss / len(test_loader.dataset)\ntop1_acc = correct1 / total\ntop5_acc = correct5 / total\n\nprint(f\"Validation Loss: {epoch_val_loss:.4f}\")\nprint(f\"Top-1 Accuracy: {top1_acc:.4f}, Top-5 Accuracy: {top5_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T22:05:09.511952Z","iopub.execute_input":"2024-12-04T22:05:09.512632Z","iopub.status.idle":"2024-12-04T22:05:49.070549Z","shell.execute_reply.started":"2024-12-04T22:05:09.512591Z","shell.execute_reply":"2024-12-04T22:05:49.069567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameters\nBATCH_SIZE = 64\nLR = 1e-3  # Learning rate for re-training\nNUM_EPOCHS = 15  # Continue training for more epochs\nSTEP_SIZE = 5\nGAMMA = 0.5  # Slightly increase decay for better convergence\n\n# Define loss function, optimizer, and learning rate scheduler\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=LR)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n\n# Re-training loop\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    running_loss = 0.0\n\n    with tqdm(train_loader, unit=\"batch\", desc=f\"Training Epoch {epoch+1}/{NUM_EPOCHS}\") as tepoch:\n        for inputs, labels in tepoch:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()  # Zero accumulated gradients\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item() * inputs.size(0)\n            tepoch.set_postfix(loss=(running_loss / len(train_loader.dataset)))\n\n    epoch_loss = running_loss / len(train_loader.dataset)\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Training Loss: {epoch_loss:.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct1 = 0\n    correct5 = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=f\"Validation Epoch {epoch+1}/{NUM_EPOCHS}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item() * inputs.size(0)\n            _, pred1 = outputs.topk(1, 1, True, True)\n            _, pred5 = outputs.topk(5, 1, True, True)\n            correct1 += pred1.eq(labels.view(-1, 1).expand_as(pred1)).sum().item()\n            correct5 += pred5.eq(labels.view(-1, 1).expand_as(pred5)).sum().item()\n            total += labels.size(0)\n\n    epoch_val_loss = val_loss / len(test_loader.dataset)\n    top1_acc = correct1 / total\n    top5_acc = correct5 / total\n\n    print(f\"Validation Loss: {epoch_val_loss:.4f}\")\n    print(f\"Top-1 Accuracy: {top1_acc:.4f}, Top-5 Accuracy: {top5_acc:.4f}\")\n\n    scheduler.step()\n\n    # Save the model after each epoch\n    torch.save(model.state_dict(), f'./cornet_s_retrained_epoch{epoch+1}.pth')\n\nprint(\"Re-training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:52:55.088127Z","iopub.execute_input":"2024-12-08T22:52:55.088423Z","iopub.status.idle":"2024-12-08T22:53:00.561987Z","shell.execute_reply.started":"2024-12-08T22:52:55.088393Z","shell.execute_reply":"2024-12-08T22:53:00.56049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torchattacks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:53:03.901824Z","iopub.execute_input":"2024-12-08T22:53:03.90257Z","iopub.status.idle":"2024-12-08T22:53:13.745482Z","shell.execute_reply.started":"2024-12-08T22:53:03.902538Z","shell.execute_reply":"2024-12-08T22:53:13.744232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of attacks with their parameters\nattacks = {\n    \"FGSM\": torchattacks.FGSM(model, eps=0.3),\n    \"PGD\": torchattacks.PGD(model, eps=0.3, alpha=2/255, steps=40),\n    \"CW\": torchattacks.CW(model, c=1e-4, kappa=0, steps=1000, lr=0.01)\n}\n\n# Evaluate model on each attack\nfor attack_name, attack in attacks.items():\n    correct = 0\n    total = 0\n    print(f\"Running {attack_name} Attack...\")\n    \n    for inputs, labels in tqdm(test_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        adv_inputs = attack(inputs, labels)  # Generate adversarial examples\n        outputs = model(adv_inputs)\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    print(f\"{attack_name} Adversarial Accuracy: {100 * correct / total:.2f}%\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T22:58:07.290307Z","iopub.execute_input":"2024-12-08T22:58:07.290949Z","iopub.status.idle":"2024-12-09T00:12:43.463597Z","shell.execute_reply.started":"2024-12-08T22:58:07.290915Z","shell.execute_reply":"2024-12-09T00:12:43.462227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport torchattacks\n\n# Ensure the model is on the correct device\nmodel.to(device)\n\n# Initialize the CW attack\ncw_attack = torchattacks.CW(model, c=1e-4, kappa=0, steps=100, lr=0.01)\n\n# Prepare to evaluate model under CW attack\ncw_correct = 0\ncw_total = 0\n\nmodel.eval()  # Set the model to evaluation mode\n\n# Iterate through the validation loader\nfor inputs, labels in tqdm(test_loader, desc=\"CW Attack Evaluation\"):\n    inputs, labels = inputs.to(device), labels.to(device)\n\n    try:\n        # Generate adversarial examples\n        adv_inputs = cw_attack(inputs, labels)\n\n        # Forward pass with adversarial examples\n        outputs = model(adv_inputs)\n\n        # Calculate accuracy\n        _, predicted = outputs.max(1)\n        cw_correct += predicted.eq(labels).sum().item()\n        cw_total += labels.size(0)\n    except Exception as e:\n        print(f\"Error during CW Attack evaluation: {e}\")\n        continue\n\n# Calculate and display CW Attack accuracy\ncw_acc = cw_correct / cw_total\nprint(f'CW Attack Accuracy: {cw_acc:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T00:13:29.7644Z","iopub.execute_input":"2024-12-09T00:13:29.764779Z","iopub.status.idle":"2024-12-09T00:37:29.634271Z","shell.execute_reply.started":"2024-12-09T00:13:29.764746Z","shell.execute_reply":"2024-12-09T00:37:29.633323Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Patch Attacks:","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
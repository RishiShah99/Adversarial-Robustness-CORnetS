{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":361017,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":300242,"modelId":320803}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-29T22:24:58.717073Z","iopub.execute_input":"2025-05-29T22:24:58.717290Z","iopub.status.idle":"2025-05-29T22:25:01.700586Z","shell.execute_reply.started":"2025-05-29T22:24:58.717271Z","shell.execute_reply":"2025-05-29T22:25:01.699735Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cornet-s_btc_mnist_50/pytorch/default/1/mnist_manual_backup_epoch50.pth\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/dicarlolab/CORnet.git\n# Navigate to the cloned repository folder\nimport os\nos.chdir('/kaggle/working/CORnet')\n\n# Install the package if needed\n!pip install .","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:07:16.547985Z","iopub.execute_input":"2025-07-03T17:07:16.548213Z","iopub.status.idle":"2025-07-03T17:08:37.273193Z","shell.execute_reply.started":"2025-07-03T17:07:16.548195Z","shell.execute_reply":"2025-07-03T17:08:37.272457Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'CORnet'...\nremote: Enumerating objects: 155, done.\u001b[K\nremote: Counting objects: 100% (20/20), done.\u001b[K\nremote: Compressing objects: 100% (12/12), done.\u001b[K\nremote: Total 155 (delta 13), reused 9 (delta 8), pack-reused 135 (from 1)\u001b[K\nReceiving objects: 100% (155/155), 68.11 KiB | 8.51 MiB/s, done.\nResolving deltas: 100% (87/87), done.\nProcessing /kaggle/working/CORnet\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (0.20.1+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (2.2.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from CORnet==0.1.0) (4.67.1)\nCollecting fire (from CORnet==0.1.0)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=0.4.0->CORnet==0.1.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=0.4.0->CORnet==0.1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=0.4.0->CORnet==0.1.0) (1.3.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->CORnet==0.1.0) (2.5.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->CORnet==0.1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->CORnet==0.1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->CORnet==0.1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->CORnet==0.1.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->CORnet==0.1.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->CORnet==0.1.0) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->CORnet==0.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->CORnet==0.1.0) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->CORnet==0.1.0) (2025.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->CORnet==0.1.0) (11.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->CORnet==0.1.0) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=0.4.0->CORnet==0.1.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->CORnet==0.1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->CORnet==0.1.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->CORnet==0.1.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->CORnet==0.1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->CORnet==0.1.0) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: CORnet, fire\n  Building wheel for CORnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for CORnet: filename=CORnet-0.1.0-py3-none-any.whl size=23228 sha256=0ee1e22cc7f6cb550beacade3149766cafe78f7e908d5f8e0aefccb53f448c10\n  Stored in directory: /tmp/pip-ephem-wheel-cache-o5jjqsqa/wheels/3d/50/75/ae6c72047fe061a7c8ef4bb49d008b40fd2b4527c39c3922e8\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=08caed38a2c94e7234ffb694be7d5d9b70faeb33f7de910a85a1499927ca1895\n  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\nSuccessfully built CORnet fire\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fire, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, CORnet\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed CORnet-0.1.0 fire-0.7.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from cornet import cornet_s\n\nmodel = cornet_s(pretrained=False)\nprint (model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:08:37.274423Z","iopub.execute_input":"2025-07-03T17:08:37.274701Z","iopub.status.idle":"2025-07-03T17:08:41.226646Z","shell.execute_reply.started":"2025-07-03T17:08:37.274667Z","shell.execute_reply":"2025-07-03T17:08:41.225888Z"}},"outputs":[{"name":"stdout","text":"DataParallel(\n  (module): Sequential(\n    (V1): Sequential(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (nonlin1): ReLU(inplace=True)\n      (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (nonlin2): ReLU(inplace=True)\n      (output): Identity()\n    )\n    (V2): CORblock_S(\n      (conv_input): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (V4): CORblock_S(\n      (conv_input): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (IT): CORblock_S(\n      (conv_input): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (skip): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (norm_skip): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv1): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin1): ReLU(inplace=True)\n      (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (nonlin2): ReLU(inplace=True)\n      (conv3): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (nonlin3): ReLU(inplace=True)\n      (output): Identity()\n      (norm1_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm1_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm2_1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (norm3_1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (decoder): Sequential(\n      (avgpool): AdaptiveAvgPool2d(output_size=1)\n      (flatten): Flatten()\n      (linear): Linear(in_features=512, out_features=1000, bias=True)\n      (output): Identity()\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport os\nfrom cornet import cornet_s\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Gaussian blur function\ndef apply_gaussian_blur(img, sigma):\n    return cv2.GaussianBlur(img, (5, 5), sigmaX=sigma, sigmaY=sigma)\n\n# Custom MNIST dataset with blur\nclass BlurryMNIST(Dataset):\n    def __init__(self, train=True, sigma=0):\n        self.dataset = torchvision.datasets.MNIST(root=\"./data\", train=train, download=True)\n        self.sigma = sigma\n        self.transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n            transforms.Normalize(mean=[0.4914, 0.4822, 0.446], std=[0.2023, 0.1994, 0.2010])\n        ])\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        img, label = self.dataset[idx]\n        img = np.array(img)\n        if self.sigma > 0:\n            img = apply_gaussian_blur(img, self.sigma)\n        img = Image.fromarray(img)\n        img = self.transform(img)\n        return img, label\n\n# CORnet-S model\nmodel = cornet_s(pretrained=False).to(device)\nmodel.module.decoder.linear = nn.Linear(model.module.decoder.linear.in_features, 10).to(device)\n\n# Training settings\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nbatch_size = 64\nnum_epochs = 10\n# Updated sigma schedule: 3 epochs with Ïƒ=2, 3 epochs with Ïƒ=1, 4 epochs with Ïƒ=0\nsigma_schedule = [2]*3 + [1]*3 + [0]*4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T22:26:35.987533Z","iopub.execute_input":"2025-05-29T22:26:35.987816Z","iopub.status.idle":"2025-05-29T22:26:47.472562Z","shell.execute_reply.started":"2025-05-29T22:26:35.987786Z","shell.execute_reply":"2025-05-29T22:26:47.471980Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --------------------- CHECKPOINT SETUP ---------------------\ncheckpoint_dir = \"./mnist_checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\ncheckpoint_path = \"/kaggle/input/mnist_epoch40/pytorch/default/1/mnist_manual_backup_epoch40.pth\"\nstart_epoch = 0\nval_acc_saved = 0.0\n\n# Load test set\ntest_dataset = BlurryMNIST(train=False, sigma=0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# --------------------- LOAD CHECKPOINT ---------------------\nif os.path.exists(checkpoint_path):\n    print(\"Resuming from checkpoint...\")\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1  # âœ… epoch = 39 â†’ start_epoch = 40\n    val_acc_saved = checkpoint.get('val_acc', 0.0)\n    print(f\"Resumed from epoch {start_epoch}, saved val acc: {val_acc_saved:.4f}\")\n\n    # Run one validation pass to confirm\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in tqdm(test_loader, desc=\"Quick Validation Check\"):\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n    val_acc_check = correct / total\n    print(f\"ğŸ” Confirmed resumed model val accuracy: {val_acc_check:.4f}\")\nelse:\n    print(\"No checkpoint found. Starting fresh.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T22:26:47.473228Z","iopub.execute_input":"2025-05-29T22:26:47.473560Z","iopub.status.idle":"2025-05-29T22:26:49.482513Z","shell.execute_reply.started":"2025-05-29T22:26:47.473541Z","shell.execute_reply":"2025-05-29T22:26:49.481820Z"}},"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 37.3MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 1.12MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 9.11MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nFailed to download (trying next):\nHTTP Error 404: Not Found\n\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\nDownloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 5.87MB/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n\nNo checkpoint found. Starting fresh.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --------------------- TRAINING LOOP ---------------------\nfor epoch in range(start_epoch, num_epochs):\n    # Inside training loop (no need to change anything else)\n    sigma = sigma_schedule[epoch]\n    print(f\"\\nEpoch {epoch+1}/{num_epochs} - Blur Ïƒ={sigma}\")\n\n    train_dataset = BlurryMNIST(train=True, sigma=sigma)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n    # Training phase\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    train_pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n\n    for images, labels in train_pbar:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n        train_pbar.set_postfix(loss=loss.item(), acc=100. * correct / total)\n\n    train_acc = correct / total\n    print(f\"Train Loss: {running_loss / len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n\n    # Validation phase\n    model.eval()\n    correct, total = 0, 0\n    val_pbar = tqdm(test_loader, desc=\"Validation\", leave=False)\n    with torch.no_grad():\n        for images, labels in val_pbar:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n            val_pbar.set_postfix(acc=100. * correct / total)\n\n    val_acc = correct / total\n    print(f\"Validation Top-1 Accuracy: {val_acc:.4f}\")\n\n    # Save checkpoint\n    save_path = os.path.join(checkpoint_dir, \"mnist_checkpoint.pth\")\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_acc': val_acc\n    }, save_path)\n    print(f\"ğŸ“¦ Checkpoint saved to {save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-29T22:26:49.483181Z","iopub.execute_input":"2025-05-29T22:26:49.483420Z","iopub.status.idle":"2025-05-30T01:04:32.535775Z","shell.execute_reply.started":"2025-05-29T22:26:49.483403Z","shell.execute_reply":"2025-05-30T01:04:32.534874Z"}},"outputs":[{"name":"stdout","text":"\nEpoch 1/10 - Blur Ïƒ=2\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.1651, Train Acc: 0.9504\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.4977\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 2/10 - Blur Ïƒ=2\n","output_type":"stream"},{"name":"stderr","text":"                                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0634, Train Acc: 0.9804\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.8705\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 3/10 - Blur Ïƒ=2\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0517, Train Acc: 0.9843\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.7146\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 4/10 - Blur Ïƒ=1\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0372, Train Acc: 0.9882\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.8308\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 5/10 - Blur Ïƒ=1\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0330, Train Acc: 0.9895\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.9545\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 6/10 - Blur Ïƒ=1\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0311, Train Acc: 0.9903\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.9783\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 7/10 - Blur Ïƒ=0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0273, Train Acc: 0.9914\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.9791\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 8/10 - Blur Ïƒ=0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0228, Train Acc: 0.9928\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.9882\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 9/10 - Blur Ïƒ=0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0219, Train Acc: 0.9930\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.9941\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n\nEpoch 10/10 - Blur Ïƒ=0\n","output_type":"stream"},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.0189, Train Acc: 0.9937\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Validation Top-1 Accuracy: 0.9936\nğŸ“¦ Checkpoint saved to ./mnist_checkpoints/mnist_checkpoint.pth\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport os\n\n# Ensure checkpoint directory exists\ncheckpoint_dir = \"./mnist_checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Path to save manual checkpoint for epoch 50\nmanual_save_path = os.path.join(checkpoint_dir, \"mnist_manual_backup_epoch10.pth\")\n\n# Set epoch to 49 since it's 0-indexed\nmanual_epoch = 9\n\n# Re-evaluate validation accuracy before saving\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        total += labels.size(0)\n\nval_acc = correct / total\nprint(f\"âœ… Final validation accuracy at epoch {manual_epoch+1}: {val_acc:.4f}\")\n\n# Save full checkpoint\ntorch.save({\n    'epoch': manual_epoch,\n    'model_state_dict': model.state_dict(),\n    'optimizer_state_dict': optimizer.state_dict(),\n    'val_acc': val_acc\n}, manual_save_path)\n\nprint(f\"ğŸ“¦ Manually saved checkpoint to: {manual_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T01:04:42.955578Z","iopub.execute_input":"2025-05-30T01:04:42.956113Z","iopub.status.idle":"2025-05-30T01:05:30.291338Z","shell.execute_reply.started":"2025-05-30T01:04:42.956083Z","shell.execute_reply":"2025-05-30T01:05:30.290665Z"}},"outputs":[{"name":"stdout","text":"âœ… Final validation accuracy at epoch 10: 0.9936\nğŸ“¦ Manually saved checkpoint to: ./mnist_checkpoints/mnist_manual_backup_epoch10.pth\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Perturbation Budgeting using CW and PGD","metadata":{}},{"cell_type":"code","source":"# Load the model\nimport torch\nfrom tqdm import tqdm\n\n# --------------------- MODEL LOADING ---------------------\n\n# Set path to your saved model\ncheckpoint_path = \"/kaggle/input/cornet-s_btc_mnist_50/pytorch/default/1/mnist_manual_backup_epoch50.pth\"\n\n# Initialize CORnet-S model for MNIST\nmodel = cornet_s(pretrained=False).to(device)\nmodel.module.decoder.linear = nn.Linear(model.module.decoder.linear.in_features, 10).to(device)\n\n# Load model checkpoint\nprint(f\"ğŸ”„ Loading model from: {checkpoint_path}\")\ncheckpoint = torch.load(checkpoint_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\nprint(f\"âœ… Model loaded successfully from epoch {checkpoint['epoch'] + 1}\")\n\n# --------------------- VALIDATE MODEL ---------------------\n\n# Load clean test set\ntest_dataset = BlurryMNIST(train=False, sigma=0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Run validation once to confirm model is good\nmodel.eval()\ncorrect, total = 0, 0\nwith torch.no_grad():\n    for images, labels in tqdm(test_loader, desc=\"Validating Loaded Model\"):\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\nval_acc = correct / total\nprint(f\"ğŸ¯ Validation Accuracy on Clean Test Set: {val_acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T20:24:44.593462Z","iopub.execute_input":"2025-04-27T20:24:44.594220Z","iopub.status.idle":"2025-04-27T20:25:36.332480Z","shell.execute_reply.started":"2025-04-27T20:24:44.594191Z","shell.execute_reply":"2025-04-27T20:25:36.331597Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Loading model from: /kaggle/input/cornet-s_btc_mnist_50/pytorch/default/1/mnist_manual_backup_epoch50.pth\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_129/2526833920.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(checkpoint_path)\n","output_type":"stream"},{"name":"stdout","text":"âœ… Model loaded successfully from epoch 50\n","output_type":"stream"},{"name":"stderr","text":"Validating Loaded Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:50<00:00,  3.14it/s]","output_type":"stream"},{"name":"stdout","text":"ğŸ¯ Validation Accuracy on Clean Test Set: 0.9926\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install torchattacks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T01:20:21.689643Z","iopub.execute_input":"2025-05-30T01:20:21.689955Z","iopub.status.idle":"2025-05-30T01:20:27.067602Z","shell.execute_reply.started":"2025-05-30T01:20:21.689932Z","shell.execute_reply":"2025-05-30T01:20:27.066596Z"}},"outputs":[{"name":"stdout","text":"Collecting torchattacks\n  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\nRequirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (2.5.1+cu124)\nRequirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (0.20.1+cu124)\nRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.15.2)\nRequirement already satisfied: tqdm>=4.56.1 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (4.67.1)\nCollecting requests~=2.25.1 (from torchattacks)\n  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: numpy>=1.19.4 in /usr/local/lib/python3.11/dist-packages (from torchattacks) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.4->torchattacks) (2.4.1)\nCollecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks)\n  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\nCollecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks)\n  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\nCollecting urllib3<1.27,>=1.21.1 (from requests~=2.25.1->torchattacks)\n  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.25.1->torchattacks) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->torchattacks) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->torchattacks) (1.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.8.2->torchattacks) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->torchattacks) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.4->torchattacks) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.4->torchattacks) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.4->torchattacks) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.4->torchattacks) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.4->torchattacks) (2024.2.0)\nDownloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: urllib3, idna, chardet, requests, torchattacks\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.3.0\n    Uninstalling urllib3-2.3.0:\n      Successfully uninstalled urllib3-2.3.0\n  Attempting uninstall: idna\n    Found existing installation: idna 3.10\n    Uninstalling idna-3.10:\n      Successfully uninstalled idna-3.10\n  Attempting uninstall: chardet\n    Found existing installation: chardet 5.2.0\n    Uninstalling chardet-5.2.0:\n      Successfully uninstalled chardet-5.2.0\n  Attempting uninstall: requests\n    Found existing installation: requests 2.32.3\n    Uninstalling requests-2.32.3:\n      Successfully uninstalled requests-2.32.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibpysal 4.9.2 requires requests>=2.27, but you have requests 2.25.1 which is incompatible.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\ndatasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ndatasets 3.5.0 requires requests>=2.32.2, but you have requests 2.25.1 which is incompatible.\njupyterlab-server 2.27.3 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\ntiktoken 0.9.0 requires requests>=2.26.0, but you have requests 2.25.1 which is incompatible.\ndocker 7.1.0 requires requests>=2.26.0, but you have requests 2.25.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.25.1 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2025.3.2 which is incompatible.\nbigframes 1.36.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nyfinance 0.2.52 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\ngoogle-genai 0.8.0 requires requests<3.0.0dev,>=2.28.1, but you have requests 2.25.1 which is incompatible.\nsphinx 8.1.3 requires requests>=2.30.0, but you have requests 2.25.1 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ntweepy 4.15.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed chardet-4.0.0 idna-2.10 requests-2.25.1 torchattacks-3.5.1 urllib3-1.26.20\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\n\n# ---------------- PGD Attack Setup ----------------\npgd_eps = 0.01         # Changeable\npgd_alpha = 2/255\npgd_steps = 40\n\npgd_attack = torchattacks.PGD(model, eps=pgd_eps, alpha=pgd_alpha, steps=pgd_steps)\n\n# ---------------- PGD Adversarial Accuracy Function ----------------\ndef adversarial_test_pgd(attack, loader):\n    model.eval()\n    correct = 0\n    total = 0\n    \n    for inputs, labels in tqdm(loader, desc=f\"Adversarial Test (PGD)\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n    \n    acc = correct / total\n    print(f\"ğŸ“Š PGD Accuracy (Îµ={pgd_eps}): {acc:.4f}\")\n    return acc\n\n# ---------------- Run PGD Evaluation ----------------\npgd_acc = adversarial_test_pgd(pgd_attack, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T02:24:04.905585Z","iopub.execute_input":"2025-05-30T02:24:04.906079Z","iopub.status.idle":"2025-05-30T03:17:33.862957Z","shell.execute_reply.started":"2025-05-30T02:24:04.906056Z","shell.execute_reply":"2025-05-30T03:17:33.862212Z"}},"outputs":[{"name":"stderr","text":"Adversarial Test (PGD): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [53:28<00:00, 20.44s/it]","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š PGD Accuracy (Îµ=0.01): 0.0958\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import torchattacks\nfrom tqdm import tqdm\n\n# ---------------- CW Attack Setup ----------------\ncw_c = 1e-3\ncw_kappa = 0\ncw_steps = 100\ncw_lr = 0.01\n\ncw_attack = torchattacks.CW(model, c=cw_c, kappa=cw_kappa, steps=cw_steps, lr=cw_lr)\n\n# ---------------- CW Adversarial Accuracy Function ----------------\ndef adversarial_test_cw(attack, loader):\n    model.eval()\n    correct = 0\n    total = 0\n\n    for inputs, labels in tqdm(loader, desc=f\"Adversarial Test (CW)\"):\n        inputs, labels = inputs.to(device), labels.to(device)\n        adv_inputs = attack(inputs, labels)\n        outputs = model(adv_inputs)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n    acc = correct / total\n    print(f\"ğŸ“Š CW Accuracy (c={cw_c}, kappa={cw_kappa}): {acc:.4f}\")\n    return acc\n\n# ---------------- Run CW Evaluation ----------------\ncw_acc = adversarial_test_cw(cw_attack, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-28T00:54:46.667100Z","iopub.execute_input":"2025-04-28T00:54:46.667814Z","iopub.status.idle":"2025-04-28T02:32:40.687397Z","shell.execute_reply.started":"2025-04-28T00:54:46.667787Z","shell.execute_reply":"2025-04-28T02:32:40.686658Z"}},"outputs":[{"name":"stderr","text":"Adversarial Test (CW): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [1:37:54<00:00, 37.41s/it]","output_type":"stream"},{"name":"stdout","text":"ğŸ“Š CW Accuracy (c=0.001, kappa=0): 0.1167\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install -q torchattacks\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}